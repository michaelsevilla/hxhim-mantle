\section{Methodology}

\subsection{ParSplice Instrumentation}
\label{sec:parsplice-instrumentation}

We instrument ParSplice with performance counters and keyspace counters.  The
performance counters track ParSplice activities while keyspace counters track
which keys are being accessed by the ranks. Because the keyspace counters have
high overhead, we only turn them on for the keyspace analysis in
Section~\S\ref{sec:parsplice-keyspace-analysis}.

The cache hierarchy is unmodified but for the backend persistent database, we
replace BerkeleyDB on NFS with LevelDB on Lustre. Original ParSplice
experiments showed that BerkeleyDB's syncs caused reads/writes to pile up on
the persistent database node. We also use Riak's customized
LevelDB\footnote{https://github.com/basho/leveldb} version, which comes
instrumented with its own set of performance counters.

\subsection{Testbed: Cray}

All experiments are run on Trinitite, a Cray XC40 with 100 nodes each with 32
Intell Haswell 2.3GHz cores. We disable hyperthreading as this has caused
problems in the past. Each node has 128GB of RAM and our goal is to limit the
size of the database to 3\% of RAM. Note that this is an addition to the 30GB
that ParSplice uses to manage other ranks on the same node. 

Inititial runs on commodity hardware, 10 CloudLab nodes and 10 nodes in our
private cluster, show poor performance compared to the Cray supercomputer. A
single Cray node produces trajectories that are \(45\times)\) longer than our
CloudLab clusters and \(24\times\) longer than our own cluster. As a result, it
reaches different job phases faster and gives us a more comprehensive view of
the workload. The performance gains compared to the commodity clusters has more
to do with memory/PCI bandwidth than network.

\subsection{Static Load Balancing}
\label{sec:static-load-balancing}

To motivate the need for load balancing we show how limiting the cache size
saves memory and sacrifices neglible performance. This type of analysis will
help inform our load balancing policies for when we switch to a distributed
key-value store backend to store segment coordinates.  We need to know when and
how to partition the keyspace: a smaller cache hurts performance because
key-value pairs need to be retrieved from other nodes while a larger cache has
higher memory usage. 

% techinical details
On each node, ParSplice uses an infinitely large cache to store segment
coordinates. We limit the size of the cache using an LRU eviction policy, where
the penalty for a cache miss is retrieving the data from the persistent
database.  We check every operation instead of when segments complete because
(1) the cache fills up too quickly, and (2) it reduces the overhead of key
eviction.

\subsection{Mantle: Dynamic Load Balancing}
\label{sec:mantle-dynamic-load-balancing}
% what is mantle
To explore dynamic load balancing policies, we use the Mantle approach.  Mantle
is a framework buit on the Ceph file system that lets admnistrators control
file system metadata load balancing policies. The basic premise is that load
balancing polcies can be expressed with a simple API consisting of when, where,
and how much callbacks. The succinctness of the API lets users inject
muitiple, dynamic policies.  

% Why is this a good idea
Although ParSplice does not use a distibuted file system, its workload is very
similiar because the minima key-value store responds to small and frequent
requests, which results in hot spots and flash crowds.  Distributed file
systems solve similiar issues: since data IO does not scale like metadata
IO~\cite{roselli:atec2000-FS-workloads}, finding optimal ways to measure,
migrate, and partition metadata load is a relatively new field, but has been
shown to lead to large performance increases and more scalable file
systems~\cite{zheng:pdsw2014-batchfs, grider:pdsw2015-marfs,
ren:sc2014-indexfs, patil:fast2011-giga+, brandt:msst2003-lh}.  Both workloads
also have data locality so the storage should have mechanisms for leveraging
requests with similar semantic meaning.  Previous work quantified the speedups
achieved with Mantle and formalized balancers that were good for file systems.

\subsection{Machine Learning Keyspace Activity}

We can't find the optimal keypsace size for every permuation, finding the key threshold changes with
\begin{itemize}
  \item number of nodes
  \item delay: Figuree~\ref{fig:futurework-regimes}
  \begin{itemize}
    \item unique keys increase over time, throughput of keys goes down
    \item smaller delay has bigger keyspace but keys are way colder 
  \end{itemize}
\end{itemize}


