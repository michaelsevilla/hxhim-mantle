\section{Related Work}

The motivation for this work is the Mochi project, which is trying to move away
from application-specific sofware stacks by pushing for re-use and composable
services.  Applications need more services to work and they are demanding a
richer set of functionality, so the hope is to share these building blocks
(e.g., Margo transport layer, SPINDLE library, MDHIM kv-store, etc.) across HPC
applications.  Malacology takes a similar approach to storage, where internal
functionality of the storage system is exposed to applications so they do not
have to bolt-on services or re-implement functionality themselves. Load
balancing is a microservice that we think would be a useful part of the Mochi
suite and Mantle seems like a good fit because it has already
identified balancers that work well for file systems. Here we try to show that
the balancers and the Mantle approach are also applicable to key-value store
workloads. 

\subsubsection*{File System Metadata} Our focus is the load balancing of a
distributed key-value store. For inspiration, we focused on file system
metadata load balancing because the hotspots and flash crowds of redundant
operations we were seeing did not match well with the keyspace partitioning
techniques of traditional databases or key-value stores.

State-of-the-art distributed file systems partition write-heavy workloads and
replicate read-heavy worklods.  IndexFS~\cite{ren:sc2014-indexfs}
partitions directories and clients write to different partitions by grabbing
leases and caching ancestor metadata for path traversal. ShardFS takes the
replication approach to the extreme by copying all directory state to all
nodes. CephFS~\ref{weil:sc2004-dyn-metadata, weil:osdi2006-ceph} employs both
techniques to a lesser extent; directories can be replicated or sharded but the
caching and replication policies are controlled with tunable parameters.
Despite the obvious benefits of these protocols, these systems still need to be
tuned by hand with {\it ad-hoc} policies designed for specific applications. 

% policies
Setting policies for migrations is arguably more difficult than adding the
migration mechanisms themselves.  For example, IndexFS and CephFS use the
GIGA+~\cite{patil:fast2011-giga} technique for partitioning directories at a
predefined threshold. Mantle makes headway in this space by providing a
framework for exploring these policies, but does not attempt anything more
sophisticated (e.g., machine learning or autotuning) to create these policies. 

% ml and autotuning
\subsubsection*{Auto-tuning} Auto-tuning is a well-known technique used in
HPC~\cite{behzad:sc2013-autotuning, behzad:techreport2014-io-autotuning}, big
data systems systems~\cite{herodotou_starfish_2011}, and
databases~\cite{schnaitter_index_2009}.  Like our work, these systems focus on
the physical design of the storage ({\it e.g.} cache size) but since we focused
on a relatively small set of parameters (cache size, migration tresholds), we
did not need anything as sophisticated as the genetic algortihm used
in~\ref{behzad:sc2013-autotuning}.  We cannot drop these techniques into
ParSplice because the magnitude and speed of the workload hotspots/flash crowds
makes existing approaches less applicable. 

\subsubsection*{Distributed KV Stores}

Our goal is to use MDHIM~\cite{greenberg:hotstorage2015-mdhim} as our back-end
key-value store because it was designed for HPC and has the proper mechanisms
for migration already implemented.  MDHIM tailors its mechanisms and policies
to HPC, showing improved performance over cloud-based key-value stores like
Cassandra~\cite{lakshman_cassandra_2010}. It has cursor types for walking the
key-value store, bulk operations for exploiting data locality, per-job server
spawning, and pluggable backends for its local database and network type
(iband/RDMA). Its policies are extenible, supporting customized partitioning
strategies and user-decondary indices.

\section{Conclusion}

Load balancing is a well-known technique for alleviating load and improving
performance, yet finding the best policies for applications is still a
hands-on, tedious process. If a load balancing module were to be accepted into
a suite of services like the Mochi project, it must transcend domains and be
useful to a wide-range of applications. In this paper, we present the framework
for such a service which (1) helps administrators explore the techniques for
informing load balancing, (2) supports dynamic policies for quickly changing
workloads and higher level intelligence engines like machine learning, and (3)
comes packaged with balancers that work well for both key-value stores and file
system metadata. We demonstrate (1) with a keyspace analysis for ParSplice and
use our findings to design a load balancing policy that improves resource
utilization.  Finally, to drive the policy engine designed in (2), we show that
a single policy is inadequate and that Mantle is flexible enough to explore
policies generated with machine learning. It is our hope that the introduction
of the Mantle approach encourages the use of machine learning and auto-tuning
for policy design in future storage systems.
